{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Docling HybridChunker Document Chunking Demo\n",
        "\n",
        "This notebook demonstrates how to use docling's HybridChunker for intelligent document chunking, suitable for Claims processing and similar scenarios.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/user/Claims_poc/myenv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "当前工作目录: /Users/user/Claims_poc/OCR\n",
            "可用文件: ['TALR7983-0923-accelerated-protection-pds-8-sep-2023.pdf', 'TAL_AcceleratedProtection_2022-08-05.pdf']\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from docling.document_converter import DocumentConverter\n",
        "from docling.chunking import HybridChunker\n",
        "import os\n",
        "import json\n",
        "\n",
        "# Check current working directory\n",
        "print(f\"Current working directory: {os.getcwd()}\")\n",
        "print(f\"Available files: {[f for f in os.listdir('.') if f.endswith('.pdf')]}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Basic Document Conversion and Chunking\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在转换文档: TALR7983-0923-accelerated-protection-pds-8-sep-2023.pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/user/Claims_poc/myenv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/Users/user/Claims_poc/myenv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/Users/user/Claims_poc/myenv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/Users/user/Claims_poc/myenv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/Users/user/Claims_poc/myenv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/Users/user/Claims_poc/myenv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        }
      ],
      "source": [
        "def convert_and_chunk_document(source_path, chunk_size=768, chunk_overlap=50):\n",
        "    \"\"\"Convert document and perform chunking\"\"\"\n",
        "    \n",
        "    # Document conversion\n",
        "    print(f\"Converting document: {source_path}\")\n",
        "    converter = DocumentConverter()\n",
        "    result = converter.convert(source_path)\n",
        "    doc = result.document\n",
        "    \n",
        "    # Configure HybridChunker - this is the core!\n",
        "    chunker = HybridChunker(\n",
        "        chunk_size=chunk_size,           # Target chunk size\n",
        "        overlap_size=chunk_overlap,      # Overlap size to maintain context continuity\n",
        "        split_by_page=True,              # Respect page boundaries\n",
        "        respect_section_boundaries=True  # Respect section boundaries\n",
        "    )\n",
        "    \n",
        "    # Execute chunking\n",
        "    chunks = chunker.chunk(doc)\n",
        "    \n",
        "    print(f\"Conversion completed! Generated {len(chunks)} chunks\")\n",
        "    \n",
        "    return doc, chunks\n",
        "\n",
        "# Example usage\n",
        "source_file = \"TALR7983-0923-accelerated-protection-pds-8-sep-2023.pdf\"\n",
        "\n",
        "if os.path.exists(source_file):\n",
        "    doc, chunks = convert_and_chunk_document(source_file, chunk_size=768, chunk_overlap=50)\n",
        "    print(f\"\\nSuccessfully processed document, generated {len(chunks)} chunks\")\n",
        "else:\n",
        "    print(f\"File {source_file} does not exist, please check the file path\")\n",
        "    print(\"You can place any PDF file in the current directory and modify the source_file variable\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. View Chunking Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def display_chunks(chunks, start_idx=0, count=3):\n",
        "    \"\"\"Display chunk content for the specified range\"\"\"\n",
        "    \n",
        "    if 'chunks' not in locals() and 'chunks' not in globals():\n",
        "        print(\"Please run the document conversion code above first\")\n",
        "        return\n",
        "    \n",
        "    print(f\"Displaying chunks {start_idx+1} to {min(start_idx + count, len(chunks))}\")\n",
        "    print(f\"Total {len(chunks)} chunks\\n\")\n",
        "    \n",
        "    for i in range(start_idx, min(start_idx + count, len(chunks))):\n",
        "        chunk = chunks[i]\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"Chunk {i+1}/{len(chunks)} (length: {len(chunk.text)} characters)\")\n",
        "        print(f\"{'='*60}\")\n",
        "        \n",
        "        # Display content (limit length for readability)\n",
        "        content = chunk.text.strip()\n",
        "        if len(content) > 300:\n",
        "            print(content[:300] + \"\\n...(content truncated, showing first 300 characters)\")\n",
        "        else:\n",
        "            print(content)\n",
        "        print()\n",
        "\n",
        "# If chunks exist, display the first 3 chunks\n",
        "try:\n",
        "    if 'chunks' in locals():\n",
        "        display_chunks(chunks, start_idx=0, count=3)\n",
        "    else:\n",
        "        print(\"Please run the document conversion code above first\")\n",
        "except NameError:\n",
        "    print(\"Please run the document conversion code above first\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Chunking Statistics Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_chunks(chunks):\n",
        "    \"\"\"Analyze chunking results\"\"\"\n",
        "    \n",
        "    chunk_lengths = [len(chunk.text) for chunk in chunks]\n",
        "    \n",
        "    print(\"📊 Chunking Statistics:\")\n",
        "    print(f\"├── Total chunks: {len(chunks)}\")\n",
        "    print(f\"├── Average length: {sum(chunk_lengths) / len(chunk_lengths):.0f} characters\")\n",
        "    print(f\"├── Maximum length: {max(chunk_lengths)} characters\")\n",
        "    print(f\"├── Minimum length: {min(chunk_lengths)} characters\")\n",
        "    \n",
        "    # Length distribution analysis\n",
        "    short_chunks = [l for l in chunk_lengths if l < 200]\n",
        "    medium_chunks = [l for l in chunk_lengths if 200 <= l <= 800]\n",
        "    long_chunks = [l for l in chunk_lengths if l > 800]\n",
        "    \n",
        "    print(f\"├── Short chunks (<200 chars): {len(short_chunks)} ({len(short_chunks)/len(chunks)*100:.1f}%)\")\n",
        "    print(f\"├── Medium chunks (200-800 chars): {len(medium_chunks)} ({len(medium_chunks)/len(chunks)*100:.1f}%)\")\n",
        "    print(f\"└── Long chunks (>800 chars): {len(long_chunks)} ({len(long_chunks)/len(chunks)*100:.1f}%)\")\n",
        "    \n",
        "    # Content type analysis\n",
        "    table_chunks = sum(1 for chunk in chunks if '|' in chunk.text or 'Table' in chunk.text)\n",
        "    list_chunks = sum(1 for chunk in chunks \n",
        "                     if any(line.strip().startswith(('-', '*', '•')) \n",
        "                           for line in chunk.text.split('\\n')))\n",
        "    \n",
        "    print(f\"\\n📋 Content Types:\")\n",
        "    print(f\"├── Chunks with tables: {table_chunks}\")\n",
        "    print(f\"└── Chunks with lists: {list_chunks}\")\n",
        "    \n",
        "    return chunk_lengths\n",
        "\n",
        "# If chunks exist, perform analysis\n",
        "try:\n",
        "    if 'chunks' in locals():\n",
        "        chunk_lengths = analyze_chunks(chunks)\n",
        "    else:\n",
        "        print(\"Please run the document conversion code first\")\n",
        "except NameError:\n",
        "    print(\"Please run the document conversion code first\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Comparison of Different Chunking Strategies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compare_chunking_strategies(doc):\n",
        "    \"\"\"Compare different chunking strategies\"\"\"\n",
        "    \n",
        "    strategies = [\n",
        "        {\"name\": \"Small chunks (fine-grained retrieval)\", \"chunk_size\": 256, \"overlap\": 25},\n",
        "        {\"name\": \"Medium chunks (balanced performance)\", \"chunk_size\": 512, \"overlap\": 50},\n",
        "        {\"name\": \"Large chunks (more context)\", \"chunk_size\": 1024, \"overlap\": 100},\n",
        "    ]\n",
        "    \n",
        "    results = {}\n",
        "    \n",
        "    print(\"🔄 Testing different chunking strategies...\")\n",
        "    \n",
        "    for strategy in strategies:\n",
        "        chunker = HybridChunker(\n",
        "            chunk_size=strategy[\"chunk_size\"],\n",
        "            overlap_size=strategy[\"overlap\"],\n",
        "            split_by_page=True,\n",
        "            respect_section_boundaries=True\n",
        "        )\n",
        "        \n",
        "        chunks = chunker.chunk(doc)\n",
        "        lengths = [len(chunk.text) for chunk in chunks]\n",
        "        \n",
        "        results[strategy[\"name\"]] = {\n",
        "            \"chunks\": chunks,\n",
        "            \"count\": len(chunks),\n",
        "            \"avg_length\": sum(lengths) / len(lengths),\n",
        "            \"lengths\": lengths\n",
        "        }\n",
        "        \n",
        "        print(f\"✓ {strategy['name']}: {len(chunks)} chunks, average length {sum(lengths) / len(lengths):.0f} characters\")\n",
        "    \n",
        "    # Display comparison table\n",
        "    print(f\"\\n📊 Strategy Comparison:\")\n",
        "    print(f\"{'Strategy':<35} {'Count':<8} {'Avg Length':<12} {'Min Length':<12} {'Max Length':<12}\")\n",
        "    print(\"-\" * 85)\n",
        "    \n",
        "    for name, result in results.items():\n",
        "        lengths = result[\"lengths\"]\n",
        "        print(f\"{name:<35} {result['count']:<8} {result['avg_length']:<12.0f} {min(lengths):<12} {max(lengths):<12}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "# If doc exists, perform strategy comparison\n",
        "try:\n",
        "    if 'doc' in locals():\n",
        "        strategy_results = compare_chunking_strategies(doc)\n",
        "    else:\n",
        "        print(\"Please run the document conversion code first\")\n",
        "except NameError:\n",
        "    print(\"Please run the document conversion code first\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Save Chunking Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_chunks_to_files(chunks, base_filename):\n",
        "    \"\"\"Save chunking results to files\"\"\"\n",
        "    \n",
        "    # Prepare structured data\n",
        "    chunk_data = []\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        chunk_info = {\n",
        "            \"id\": i,\n",
        "            \"text\": chunk.text.strip(),\n",
        "            \"length\": len(chunk.text),\n",
        "            \"word_count\": len(chunk.text.split()),\n",
        "            \"has_tables\": \"|\" in chunk.text or \"Table\" in chunk.text,\n",
        "            \"has_lists\": any(line.strip().startswith((\"-\", \"*\", \"•\")) \n",
        "                           for line in chunk.text.split(\"\\n\")),\n",
        "            \"metadata\": {\n",
        "                \"chunk_type\": \"table\" if \"|\" in chunk.text else \n",
        "                             \"list\" if any(line.strip().startswith((\"-\", \"*\", \"•\")) \n",
        "                                         for line in chunk.text.split(\"\\n\")) else \"text\"\n",
        "            }\n",
        "        }\n",
        "        chunk_data.append(chunk_info)\n",
        "    \n",
        "    # Save JSON format (for programmatic processing)\n",
        "    json_filename = f\"{base_filename}_chunks.json\"\n",
        "    with open(json_filename, 'w', encoding='utf-8') as f:\n",
        "        json.dump(chunk_data, f, ensure_ascii=False, indent=2)\n",
        "    \n",
        "    # Save readable format (for human review)\n",
        "    txt_filename = f\"{base_filename}_chunks.txt\"\n",
        "    with open(txt_filename, 'w', encoding='utf-8') as f:\n",
        "        f.write(f\"Document Chunking Results\\\\n{'='*50}\\\\n\\\\n\")\n",
        "        f.write(f\"Original document: {base_filename}\\\\n\")\n",
        "        f.write(f\"Total chunks: {len(chunks)}\\\\n\")\n",
        "        f.write(f\"Average chunk size: {sum(c['length'] for c in chunk_data) / len(chunk_data):.0f} characters\\\\n\\\\n\")\n",
        "        \n",
        "        for chunk_info in chunk_data:\n",
        "            f.write(f\"--- Chunk {chunk_info['id'] + 1} [{chunk_info['metadata']['chunk_type']}] ---\\\\n\")\n",
        "            f.write(f\"Length: {chunk_info['length']} characters, {chunk_info['word_count']} words\\\\n\")\n",
        "            if chunk_info['has_tables']:\n",
        "                f.write(\"✓ Contains tables\\\\n\")\n",
        "            if chunk_info['has_lists']:\n",
        "                f.write(\"✓ Contains lists\\\\n\")\n",
        "            f.write(f\"Content:\\\\n{chunk_info['text']}\\\\n\\\\n\")\n",
        "    \n",
        "    print(f\"💾 Chunking results saved:\")\n",
        "    print(f\"├── JSON format: {json_filename}\")\n",
        "    print(f\"└── Text format: {txt_filename}\")\n",
        "    \n",
        "    return json_filename, txt_filename\n",
        "\n",
        "# If chunks exist, save results\n",
        "try:\n",
        "    if 'chunks' in locals() and 'source_file' in locals():\n",
        "        base_name = os.path.splitext(source_file)[0]\n",
        "        json_file, txt_file = save_chunks_to_files(chunks, base_name)\n",
        "    else:\n",
        "        print(\"Please run the document conversion code first\")\n",
        "except NameError:\n",
        "    print(\"Please run the document conversion code first\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. Advanced Usage for Claims Processing\n",
        "\n",
        "For insurance Claims documents, we can perform more specialized content analysis:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_claims_content(chunks):\n",
        "    \"\"\"Analyze Claims-related content types\"\"\"\n",
        "    \n",
        "    # Define keyword categories\n",
        "    claims_keywords = [\n",
        "        'claim', 'policy', 'premium', 'coverage', 'deductible',\n",
        "        'benefit', 'exclusion', 'liability', 'settlement', 'payout',\n",
        "        'insured', 'policyholder', 'beneficiary'\n",
        "    ]\n",
        "    \n",
        "    financial_keywords = [\n",
        "        '$', 'amount', 'cost', 'fee', 'rate', 'percentage', '%',\n",
        "        'sum', 'limit', 'maximum', 'minimum'\n",
        "    ]\n",
        "    \n",
        "    legal_keywords = [\n",
        "        'terms', 'conditions', 'clause', 'provision', 'agreement',\n",
        "        'contract', 'obligation', 'responsibility', 'liable'\n",
        "    ]\n",
        "    \n",
        "    results = {\n",
        "        'claims_related': [],\n",
        "        'financial_info': [],\n",
        "        'legal_terms': [],\n",
        "        'tables_and_data': [],\n",
        "        'general_text': []\n",
        "    }\n",
        "    \n",
        "    print(\"🔍 Analyzing Claims document content...\")\n",
        "    \n",
        "    for i, chunk in enumerate(chunks):\n",
        "        text_lower = chunk.text.lower()\n",
        "        \n",
        "        # Classify chunks\n",
        "        has_claims = any(keyword in text_lower for keyword in claims_keywords)\n",
        "        has_financial = any(keyword in text_lower for keyword in financial_keywords)\n",
        "        has_legal = any(keyword in text_lower for keyword in legal_keywords)\n",
        "        has_tables = '|' in chunk.text or 'Table' in chunk.text\n",
        "        \n",
        "        chunk_info = {\n",
        "            'index': i,\n",
        "            'length': len(chunk.text),\n",
        "            'preview': chunk.text[:80] + '...' if len(chunk.text) > 80 else chunk.text,\n",
        "            'keywords_found': []\n",
        "        }\n",
        "        \n",
        "        # Record found keywords\n",
        "        if has_claims:\n",
        "            found_claims = [kw for kw in claims_keywords if kw in text_lower]\n",
        "            chunk_info['keywords_found'].extend(found_claims)\n",
        "            results['claims_related'].append(chunk_info.copy())\n",
        "        \n",
        "        if has_financial:\n",
        "            found_financial = [kw for kw in financial_keywords if kw in text_lower]\n",
        "            chunk_info['keywords_found'].extend(found_financial)\n",
        "            results['financial_info'].append(chunk_info.copy())\n",
        "        \n",
        "        if has_legal:\n",
        "            found_legal = [kw for kw in legal_keywords if kw in text_lower]\n",
        "            chunk_info['keywords_found'].extend(found_legal)\n",
        "            results['legal_terms'].append(chunk_info.copy())\n",
        "        \n",
        "        if has_tables:\n",
        "            results['tables_and_data'].append(chunk_info.copy())\n",
        "        \n",
        "        if not (has_claims or has_financial or has_legal or has_tables):\n",
        "            results['general_text'].append(chunk_info.copy())\n",
        "    \n",
        "    # Print statistics\n",
        "    print(f\"\\\\n📋 Claims Content Analysis Results:\")\n",
        "    print(f\"├── Claims-related chunks: {len(results['claims_related'])}\")\n",
        "    print(f\"├── Financial info chunks: {len(results['financial_info'])}\")\n",
        "    print(f\"├── Legal terms chunks: {len(results['legal_terms'])}\")\n",
        "    print(f\"├── Tables/data chunks: {len(results['tables_and_data'])}\")\n",
        "    print(f\"└── General text chunks: {len(results['general_text'])}\")\n",
        "    \n",
        "    # Show some key chunk previews\n",
        "    if results['claims_related']:\n",
        "        print(f\"\\\\n🏷️  Claims-related chunk examples:\")\n",
        "        for chunk_info in results['claims_related'][:2]:\n",
        "            print(f\"  Chunk {chunk_info['index']+1}: {chunk_info['preview']}\")\n",
        "    \n",
        "    if results['financial_info']:\n",
        "        print(f\"\\\\n💰 Financial info chunk examples:\")\n",
        "        for chunk_info in results['financial_info'][:2]:\n",
        "            print(f\"  Chunk {chunk_info['index']+1}: {chunk_info['preview']}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "# If chunks exist, perform Claims analysis\n",
        "try:\n",
        "    if 'chunks' in locals():\n",
        "        claims_analysis = analyze_claims_content(chunks)\n",
        "    else:\n",
        "        print(\"Please run the document conversion code first\")\n",
        "except NameError:\n",
        "    print(\"Please run the document conversion code first\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
