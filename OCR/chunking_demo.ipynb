{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Docling HybridChunker Document Chunking Demo\n",
        "\n",
        "This notebook demonstrates how to use docling's HybridChunker for intelligent document chunking, suitable for Claims processing and similar scenarios.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current working directory: /Users/tianying/Claims_poc/ocr\n",
            "Available files: ['2025 Data engineer Resume.pdf', 'TALR7983-0923-accelerated-protection-pds-8-sep-2023.pdf', 'TAL_AcceleratedProtection_2022-08-05.pdf']\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from docling.document_converter import DocumentConverter\n",
        "from docling.chunking import HybridChunker\n",
        "import os\n",
        "import json\n",
        "\n",
        "# Check current working directory\n",
        "print(f\"Current working directory: {os.getcwd()}\")\n",
        "print(f\"Available files: {[f for f in os.listdir('.') if f.endswith('.pdf')]}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Basic Document Conversion and Chunking\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converting document: 2025 Data engineer Resume.pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/tianying/Claims_poc/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Conversion completed! Generated 6 chunks\n",
            "\n",
            "Successfully processed document, generated 6 chunks\n"
          ]
        }
      ],
      "source": [
        "def convert_and_chunk_document(source_path, chunk_size=768, chunk_overlap=50):\n",
        "    \"\"\"Convert document and perform chunking\"\"\"\n",
        "    \n",
        "    # Document conversion\n",
        "    print(f\"Converting document: {source_path}\")\n",
        "    converter = DocumentConverter()\n",
        "    result = converter.convert(source_path)\n",
        "    doc = result.document\n",
        "    \n",
        "    # Configure HybridChunker - this is the core!\n",
        "    chunker = HybridChunker(\n",
        "        chunk_size=chunk_size,           # Target chunk size\n",
        "        overlap_size=chunk_overlap,      # Overlap size to maintain context continuity\n",
        "        split_by_page=True,              # Respect page boundaries\n",
        "        respect_section_boundaries=True  # Respect section boundaries\n",
        "    )\n",
        "    \n",
        "    # Execute chunking\n",
        "    chunks = list(chunker.chunk(doc))\n",
        "\n",
        "    print(f\"Conversion completed! Generated {len(chunks)} chunks\")\n",
        "    \n",
        "    return doc, chunks\n",
        "\n",
        "# Example usage\n",
        "source_file = \"2025 Data engineer Resume.pdf\"\n",
        "\n",
        "if os.path.exists(source_file):\n",
        "    doc, chunks = convert_and_chunk_document(source_file, chunk_size=768, chunk_overlap=50)\n",
        "    print(f\"\\nSuccessfully processed document, generated {len(chunks)} chunks\")\n",
        "else:\n",
        "    print(f\"File {source_file} does not exist, please check the file path\")\n",
        "    print(\"You can place any PDF file in the current directory and modify the source_file variable\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DocChunk(text='Data Modelling, Data Warehousing, Data Lakes, Relational Databases, File Storage, APIs, Modern Lakehouses, Cloud Environments, Data Engineering, Data Analytics, Data Preparation, Data Transformation, Data Integration, Data Governance, Data Quality, Data Security, Data Visualisation, Data Sense Making, Data Manipulation, Data Structures, Data Mining, ETL (Extract, Transform, Load), Data Vault Modelling, Batch Processing, Real-time Processing, Problem-solving, Troubleshooting, Optimise Performance, Collaboration, Communication, Stakeholder\\nEngagement, Business Intelligence (BI), User Behaviour Analytics, Experimentation, High Growth Mindset, Agile Methodology, Cloud Infrastructure, SaaS Platforms, Intellectual Curiosity, Results-oriented, Adaptability, Ambiguity, Fast-paced Environment, Change, Financial Services, Financial Markets Literacy, Capital Markets, Asset Classes, Equities, Derivatives, Fixed Income, SQL, Python, Java, Scala, Spark, PySpark, DBT (Data Build Tool), Databricks, Hadoop, Kafka, Snowflake, BigQuery, Azure (Azure Data Lake, Azure SQL Database, Azure Synapse Analytics,', meta=DocMeta(schema_name='docling_core.transforms.chunker.DocMeta', version='1.0.0', doc_items=[DocItem(self_ref='#/texts/50', parent=RefItem(cref='#/body'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=1, bbox=BoundingBox(l=36.0, t=55.347999999999956, r=547.147, b=53.49699999999996, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 543))]), DocItem(self_ref='#/texts/51', parent=RefItem(cref='#/body'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=1, bbox=BoundingBox(l=36.0, t=52.76099999999997, r=558.283, b=50.90899999999999, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 558))])], headings=['Data Analyst Intern'], captions=None, origin=DocumentOrigin(mimetype='application/pdf', binary_hash=12323231099123879602, filename='2025 Data engineer Resume.pdf', uri=None)))"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chunks[4]"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. View Chunking Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def display_chunks(chunks, start_idx=0, count=3):\n",
        "    \"\"\"Display chunk content for the specified range\"\"\"\n",
        "    \n",
        "    if 'chunks' not in locals() and 'chunks' not in globals():\n",
        "        print(\"Please run the document conversion code above first\")\n",
        "        return\n",
        "    \n",
        "    print(f\"Displaying chunks {start_idx+1} to {min(start_idx + count, len(chunks))}\")\n",
        "    print(f\"Total {len(chunks)} chunks\\n\")\n",
        "    \n",
        "    for i in range(start_idx, min(start_idx + count, len(chunks))):\n",
        "        chunk = chunks[i]\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"Chunk {i+1}/{len(chunks)} (length: {len(chunk.text)} characters)\")\n",
        "        print(f\"{'='*60}\")\n",
        "        \n",
        "        # Display content (limit length for readability)\n",
        "        content = chunk.text.strip()\n",
        "        if len(content) > 300:\n",
        "            print(content[:300] + \"\\n...(content truncated, showing first 300 characters)\")\n",
        "        else:\n",
        "            print(content)\n",
        "        print()\n",
        "\n",
        "# If chunks exist, display the first 3 chunks\n",
        "try:\n",
        "    if 'chunks' in locals():\n",
        "        display_chunks(chunks, start_idx=0, count=3)\n",
        "    else:\n",
        "        print(\"Please run the document conversion code above first\")\n",
        "except NameError:\n",
        "    print(\"Please run the document conversion code above first\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Chunking Statistics Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_chunks(chunks):\n",
        "    \"\"\"Analyze chunking results\"\"\"\n",
        "    \n",
        "    chunk_lengths = [len(chunk.text) for chunk in chunks]\n",
        "    \n",
        "    print(\"üìä Chunking Statistics:\")\n",
        "    print(f\"‚îú‚îÄ‚îÄ Total chunks: {len(chunks)}\")\n",
        "    print(f\"‚îú‚îÄ‚îÄ Average length: {sum(chunk_lengths) / len(chunk_lengths):.0f} characters\")\n",
        "    print(f\"‚îú‚îÄ‚îÄ Maximum length: {max(chunk_lengths)} characters\")\n",
        "    print(f\"‚îú‚îÄ‚îÄ Minimum length: {min(chunk_lengths)} characters\")\n",
        "    \n",
        "    # Length distribution analysis\n",
        "    short_chunks = [l for l in chunk_lengths if l < 200]\n",
        "    medium_chunks = [l for l in chunk_lengths if 200 <= l <= 800]\n",
        "    long_chunks = [l for l in chunk_lengths if l > 800]\n",
        "    \n",
        "    print(f\"‚îú‚îÄ‚îÄ Short chunks (<200 chars): {len(short_chunks)} ({len(short_chunks)/len(chunks)*100:.1f}%)\")\n",
        "    print(f\"‚îú‚îÄ‚îÄ Medium chunks (200-800 chars): {len(medium_chunks)} ({len(medium_chunks)/len(chunks)*100:.1f}%)\")\n",
        "    print(f\"‚îî‚îÄ‚îÄ Long chunks (>800 chars): {len(long_chunks)} ({len(long_chunks)/len(chunks)*100:.1f}%)\")\n",
        "    \n",
        "    # Content type analysis\n",
        "    table_chunks = sum(1 for chunk in chunks if '|' in chunk.text or 'Table' in chunk.text)\n",
        "    list_chunks = sum(1 for chunk in chunks \n",
        "                     if any(line.strip().startswith(('-', '*', '‚Ä¢')) \n",
        "                           for line in chunk.text.split('\\n')))\n",
        "    \n",
        "    print(f\"\\nüìã Content Types:\")\n",
        "    print(f\"‚îú‚îÄ‚îÄ Chunks with tables: {table_chunks}\")\n",
        "    print(f\"‚îî‚îÄ‚îÄ Chunks with lists: {list_chunks}\")\n",
        "    \n",
        "    return chunk_lengths\n",
        "\n",
        "# If chunks exist, perform analysis\n",
        "try:\n",
        "    if 'chunks' in locals():\n",
        "        chunk_lengths = analyze_chunks(chunks)\n",
        "    else:\n",
        "        print(\"Please run the document conversion code first\")\n",
        "except NameError:\n",
        "    print(\"Please run the document conversion code first\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Comparison of Different Chunking Strategies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compare_chunking_strategies(doc):\n",
        "    \"\"\"Compare different chunking strategies\"\"\"\n",
        "    \n",
        "    strategies = [\n",
        "        {\"name\": \"Small chunks (fine-grained retrieval)\", \"chunk_size\": 256, \"overlap\": 25},\n",
        "        {\"name\": \"Medium chunks (balanced performance)\", \"chunk_size\": 512, \"overlap\": 50},\n",
        "        {\"name\": \"Large chunks (more context)\", \"chunk_size\": 1024, \"overlap\": 100},\n",
        "    ]\n",
        "    \n",
        "    results = {}\n",
        "    \n",
        "    print(\"üîÑ Testing different chunking strategies...\")\n",
        "    \n",
        "    for strategy in strategies:\n",
        "        chunker = HybridChunker(\n",
        "            chunk_size=strategy[\"chunk_size\"],\n",
        "            overlap_size=strategy[\"overlap\"],\n",
        "            split_by_page=True,\n",
        "            respect_section_boundaries=True\n",
        "        )\n",
        "        \n",
        "        chunks = chunker.chunk(doc)\n",
        "        lengths = [len(chunk.text) for chunk in chunks]\n",
        "        \n",
        "        results[strategy[\"name\"]] = {\n",
        "            \"chunks\": chunks,\n",
        "            \"count\": len(chunks),\n",
        "            \"avg_length\": sum(lengths) / len(lengths),\n",
        "            \"lengths\": lengths\n",
        "        }\n",
        "        \n",
        "        print(f\"‚úì {strategy['name']}: {len(chunks)} chunks, average length {sum(lengths) / len(lengths):.0f} characters\")\n",
        "    \n",
        "    # Display comparison table\n",
        "    print(f\"\\nüìä Strategy Comparison:\")\n",
        "    print(f\"{'Strategy':<35} {'Count':<8} {'Avg Length':<12} {'Min Length':<12} {'Max Length':<12}\")\n",
        "    print(\"-\" * 85)\n",
        "    \n",
        "    for name, result in results.items():\n",
        "        lengths = result[\"lengths\"]\n",
        "        print(f\"{name:<35} {result['count']:<8} {result['avg_length']:<12.0f} {min(lengths):<12} {max(lengths):<12}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "# If doc exists, perform strategy comparison\n",
        "try:\n",
        "    if 'doc' in locals():\n",
        "        strategy_results = compare_chunking_strategies(doc)\n",
        "    else:\n",
        "        print(\"Please run the document conversion code first\")\n",
        "except NameError:\n",
        "    print(\"Please run the document conversion code first\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Save Chunking Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_chunks_to_files(chunks, base_filename):\n",
        "    \"\"\"Save chunking results to files\"\"\"\n",
        "    \n",
        "    # Prepare structured data\n",
        "    chunk_data = []\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        chunk_info = {\n",
        "            \"id\": i,\n",
        "            \"text\": chunk.text.strip(),\n",
        "            \"length\": len(chunk.text),\n",
        "            \"word_count\": len(chunk.text.split()),\n",
        "            \"has_tables\": \"|\" in chunk.text or \"Table\" in chunk.text,\n",
        "            \"has_lists\": any(line.strip().startswith((\"-\", \"*\", \"‚Ä¢\")) \n",
        "                           for line in chunk.text.split(\"\\n\")),\n",
        "            \"metadata\": {\n",
        "                \"chunk_type\": \"table\" if \"|\" in chunk.text else \n",
        "                             \"list\" if any(line.strip().startswith((\"-\", \"*\", \"‚Ä¢\")) \n",
        "                                         for line in chunk.text.split(\"\\n\")) else \"text\"\n",
        "            }\n",
        "        }\n",
        "        chunk_data.append(chunk_info)\n",
        "    \n",
        "    # Save JSON format (for programmatic processing)\n",
        "    json_filename = f\"{base_filename}_chunks.json\"\n",
        "    with open(json_filename, 'w', encoding='utf-8') as f:\n",
        "        json.dump(chunk_data, f, ensure_ascii=False, indent=2)\n",
        "    \n",
        "    # Save readable format (for human review)\n",
        "    txt_filename = f\"{base_filename}_chunks.txt\"\n",
        "    with open(txt_filename, 'w', encoding='utf-8') as f:\n",
        "        f.write(f\"Document Chunking Results\\\\n{'='*50}\\\\n\\\\n\")\n",
        "        f.write(f\"Original document: {base_filename}\\\\n\")\n",
        "        f.write(f\"Total chunks: {len(chunks)}\\\\n\")\n",
        "        f.write(f\"Average chunk size: {sum(c['length'] for c in chunk_data) / len(chunk_data):.0f} characters\\\\n\\\\n\")\n",
        "        \n",
        "        for chunk_info in chunk_data:\n",
        "            f.write(f\"--- Chunk {chunk_info['id'] + 1} [{chunk_info['metadata']['chunk_type']}] ---\\\\n\")\n",
        "            f.write(f\"Length: {chunk_info['length']} characters, {chunk_info['word_count']} words\\\\n\")\n",
        "            if chunk_info['has_tables']:\n",
        "                f.write(\"‚úì Contains tables\\\\n\")\n",
        "            if chunk_info['has_lists']:\n",
        "                f.write(\"‚úì Contains lists\\\\n\")\n",
        "            f.write(f\"Content:\\\\n{chunk_info['text']}\\\\n\\\\n\")\n",
        "    \n",
        "    print(f\"üíæ Chunking results saved:\")\n",
        "    print(f\"‚îú‚îÄ‚îÄ JSON format: {json_filename}\")\n",
        "    print(f\"‚îî‚îÄ‚îÄ Text format: {txt_filename}\")\n",
        "    \n",
        "    return json_filename, txt_filename\n",
        "\n",
        "# If chunks exist, save results\n",
        "try:\n",
        "    if 'chunks' in locals() and 'source_file' in locals():\n",
        "        base_name = os.path.splitext(source_file)[0]\n",
        "        json_file, txt_file = save_chunks_to_files(chunks, base_name)\n",
        "    else:\n",
        "        print(\"Please run the document conversion code first\")\n",
        "except NameError:\n",
        "    print(\"Please run the document conversion code first\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. Advanced Usage for Claims Processing\n",
        "\n",
        "For insurance Claims documents, we can perform more specialized content analysis:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_claims_content(chunks):\n",
        "    \"\"\"Analyze Claims-related content types\"\"\"\n",
        "    \n",
        "    # Define keyword categories\n",
        "    claims_keywords = [\n",
        "        'claim', 'policy', 'premium', 'coverage', 'deductible',\n",
        "        'benefit', 'exclusion', 'liability', 'settlement', 'payout',\n",
        "        'insured', 'policyholder', 'beneficiary'\n",
        "    ]\n",
        "    \n",
        "    financial_keywords = [\n",
        "        '$', 'amount', 'cost', 'fee', 'rate', 'percentage', '%',\n",
        "        'sum', 'limit', 'maximum', 'minimum'\n",
        "    ]\n",
        "    \n",
        "    legal_keywords = [\n",
        "        'terms', 'conditions', 'clause', 'provision', 'agreement',\n",
        "        'contract', 'obligation', 'responsibility', 'liable'\n",
        "    ]\n",
        "    \n",
        "    results = {\n",
        "        'claims_related': [],\n",
        "        'financial_info': [],\n",
        "        'legal_terms': [],\n",
        "        'tables_and_data': [],\n",
        "        'general_text': []\n",
        "    }\n",
        "    \n",
        "    print(\"üîç Analyzing Claims document content...\")\n",
        "    \n",
        "    for i, chunk in enumerate(chunks):\n",
        "        text_lower = chunk.text.lower()\n",
        "        \n",
        "        # Classify chunks\n",
        "        has_claims = any(keyword in text_lower for keyword in claims_keywords)\n",
        "        has_financial = any(keyword in text_lower for keyword in financial_keywords)\n",
        "        has_legal = any(keyword in text_lower for keyword in legal_keywords)\n",
        "        has_tables = '|' in chunk.text or 'Table' in chunk.text\n",
        "        \n",
        "        chunk_info = {\n",
        "            'index': i,\n",
        "            'length': len(chunk.text),\n",
        "            'preview': chunk.text[:80] + '...' if len(chunk.text) > 80 else chunk.text,\n",
        "            'keywords_found': []\n",
        "        }\n",
        "        \n",
        "        # Record found keywords\n",
        "        if has_claims:\n",
        "            found_claims = [kw for kw in claims_keywords if kw in text_lower]\n",
        "            chunk_info['keywords_found'].extend(found_claims)\n",
        "            results['claims_related'].append(chunk_info.copy())\n",
        "        \n",
        "        if has_financial:\n",
        "            found_financial = [kw for kw in financial_keywords if kw in text_lower]\n",
        "            chunk_info['keywords_found'].extend(found_financial)\n",
        "            results['financial_info'].append(chunk_info.copy())\n",
        "        \n",
        "        if has_legal:\n",
        "            found_legal = [kw for kw in legal_keywords if kw in text_lower]\n",
        "            chunk_info['keywords_found'].extend(found_legal)\n",
        "            results['legal_terms'].append(chunk_info.copy())\n",
        "        \n",
        "        if has_tables:\n",
        "            results['tables_and_data'].append(chunk_info.copy())\n",
        "        \n",
        "        if not (has_claims or has_financial or has_legal or has_tables):\n",
        "            results['general_text'].append(chunk_info.copy())\n",
        "    \n",
        "    # Print statistics\n",
        "    print(f\"\\\\nüìã Claims Content Analysis Results:\")\n",
        "    print(f\"‚îú‚îÄ‚îÄ Claims-related chunks: {len(results['claims_related'])}\")\n",
        "    print(f\"‚îú‚îÄ‚îÄ Financial info chunks: {len(results['financial_info'])}\")\n",
        "    print(f\"‚îú‚îÄ‚îÄ Legal terms chunks: {len(results['legal_terms'])}\")\n",
        "    print(f\"‚îú‚îÄ‚îÄ Tables/data chunks: {len(results['tables_and_data'])}\")\n",
        "    print(f\"‚îî‚îÄ‚îÄ General text chunks: {len(results['general_text'])}\")\n",
        "    \n",
        "    # Show some key chunk previews\n",
        "    if results['claims_related']:\n",
        "        print(f\"\\\\nüè∑Ô∏è  Claims-related chunk examples:\")\n",
        "        for chunk_info in results['claims_related'][:2]:\n",
        "            print(f\"  Chunk {chunk_info['index']+1}: {chunk_info['preview']}\")\n",
        "    \n",
        "    if results['financial_info']:\n",
        "        print(f\"\\\\nüí∞ Financial info chunk examples:\")\n",
        "        for chunk_info in results['financial_info'][:2]:\n",
        "            print(f\"  Chunk {chunk_info['index']+1}: {chunk_info['preview']}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "# If chunks exist, perform Claims analysis\n",
        "try:\n",
        "    if 'chunks' in locals():\n",
        "        claims_analysis = analyze_claims_content(chunks)\n",
        "    else:\n",
        "        print(\"Please run the document conversion code first\")\n",
        "except NameError:\n",
        "    print(\"Please run the document conversion code first\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 11, 'total_tokens': 20, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-Bjkevn7QYjIRgWLwruSULHhasV8lM', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--8a0db2e4-33fd-408d-907c-38daee434880-0', usage_metadata={'input_tokens': 11, 'output_tokens': 9, 'total_tokens': 20, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI()\n",
        "llm.invoke(\"Hello, world!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
