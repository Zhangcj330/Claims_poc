#!/usr/bin/env python3
"""
OCR Chunks Review - è¿è¡Œè„šæœ¬

è¿™ä¸ªè„šæœ¬æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ ChunksReviewer æ¥æ£€æŸ¥å’Œæ”¹è¿›ç°æœ‰çš„ OCR chunksã€‚

ä½¿ç”¨æ–¹æ³•:
    python run_chunks_review.py

é…ç½®æ–‡ä»¶:
    è¯·ç¡®ä¿åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®äº† GOOGLE_API_KEY
"""

import os
import sys
import argparse
from pathlib import Path
from dotenv import load_dotenv

# æ·»åŠ å½“å‰ç›®å½•åˆ° Python è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from chunks_reviewer import ChunksReviewer

def main():
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='Review and improve OCR chunks using Gemini 2.5')
    parser.add_argument('--jsonl', '-j', 
                       default='Data_Json/TAL_AcceleratedProtection_2022-08-05_chunks.jsonl',
                       help='Path to input JSONL chunks file')
    parser.add_argument('--pdf', '-p',
                       default='TAL_AcceleratedProtection_2022-08-05.pdf', 
                       help='Path to original PDF file')
    parser.add_argument('--output', '-o',
                       help='Output directory (default: same as input)')
    parser.add_argument('--threshold', '-t', type=float, default=75.0,
                       help='Confidence threshold (default: 75.0)')
    parser.add_argument('--dry-run', action='store_true',
                       help='Only show file information without processing')
    
    args = parser.parse_args()
    
    # åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv()
    
    # æ£€æŸ¥ API key
    if not os.getenv('GOOGLE_API_KEY'):
        print("âŒ é”™è¯¯: è¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½® GOOGLE_API_KEY")
        print("ğŸ’¡ æç¤º: åœ¨é¡¹ç›®æ ¹ç›®å½•åˆ›å»º .env æ–‡ä»¶ï¼Œå†…å®¹å¦‚ä¸‹:")
        print("   GOOGLE_API_KEY=your_api_key_here")
        return 1
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    jsonl_path = args.jsonl
    pdf_path = args.pdf
    
    print("ğŸš€ OCR Chunks Review System")
    print("=" * 50)
    print(f"ğŸ“„ JSONL æ–‡ä»¶: {jsonl_path}")
    print(f"ğŸ“„ PDF æ–‡ä»¶: {pdf_path}")
    print(f"ğŸ¯ ç½®ä¿¡åº¦é˜ˆå€¼: {args.threshold}")
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(jsonl_path):
        print(f"âŒ é”™è¯¯: JSONL æ–‡ä»¶ä¸å­˜åœ¨: {jsonl_path}")
        return 1
    
    if not os.path.exists(pdf_path):
        print(f"âŒ é”™è¯¯: PDF æ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}")
        return 1
    
    print("âœ… è¾“å…¥æ–‡ä»¶æ£€æŸ¥é€šè¿‡")
    
    # å¦‚æœæ˜¯ dry runï¼Œåªæ˜¾ç¤ºæ–‡ä»¶ä¿¡æ¯
    if args.dry_run:
        print("\nğŸ” Dry Run - æ–‡ä»¶ä¿¡æ¯:")
        
        # åˆå§‹åŒ– reviewerï¼ˆä¸è°ƒç”¨ APIï¼‰
        try:
            reviewer = ChunksReviewer()
        except Exception as e:
            print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
            return 1
        
        # åŠ è½½ chunks
        chunks = reviewer.load_chunks_from_jsonl(jsonl_path)
        page_groups = reviewer.group_chunks_by_page(chunks)
        
        print(f"ğŸ“Š æ€» chunks: {len(chunks)}")
        print(f"ğŸ“„ æ€»é¡µé¢æ•°: {len(page_groups)}")
        print(f"ğŸ“„ é¡µé¢èŒƒå›´: {min(page_groups.keys())} - {max(page_groups.keys())}")
        
        # æ˜¾ç¤ºæ¯é¡µçš„ chunks æ•°é‡
        print("\nğŸ“„ å„é¡µé¢ chunks åˆ†å¸ƒ:")
        for page_no in sorted(page_groups.keys())[:10]:  # åªæ˜¾ç¤ºå‰10é¡µ
            chunk_count = len(page_groups[page_no])
            print(f"   ç¬¬ {page_no:2d} é¡µ: {chunk_count} chunks")
        
        if len(page_groups) > 10:
            print(f"   ... è¿˜æœ‰ {len(page_groups) - 10} é¡µ")
        
        print("\nğŸ’¡ è¦å¼€å§‹å®é™…å¤„ç†ï¼Œè¯·è¿è¡Œä¸å¸¦ --dry-run å‚æ•°çš„å‘½ä»¤")
        return 0
    
    # å®é™…å¤„ç†
    try:
        print("\nğŸš€ å¼€å§‹å¤„ç†...")
        
        # åˆå§‹åŒ– reviewer
        reviewer = ChunksReviewer()
        print("âœ… ChunksReviewer åˆå§‹åŒ–æˆåŠŸ")
        
        # è¿è¡Œå®¡æŸ¥å’Œæ”¹è¿›
        results = reviewer.review_and_improve_chunks(
            jsonl_path=jsonl_path,
            pdf_path=pdf_path,
            output_dir=args.output,
            confidence_threshold=args.threshold
        )
        
        # æ˜¾ç¤ºç»“æœ
        print("\nğŸ‰ å¤„ç†å®Œæˆ!")
        summary = results['summary']
        
        print("\nğŸ“Š ç»“æœæ‘˜è¦:")
        print("=" * 30)
        print(f"ğŸ“„ æ€»é¡µé¢æ•°: {summary['total_pages']}")
        print(f"ğŸ”§ æ”¹è¿›é¡µé¢æ•°: {summary['pages_improved']}")
        print(f"ğŸ“ˆ æ”¹è¿›ç‡: {summary['improvement_rate']*100:.1f}%")
        print(f"â­ å¹³å‡ç½®ä¿¡åº¦: {summary['average_confidence']:.1f}")
        
        print("\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
        files = results['files']
        print(f"ğŸ“„ æ”¹è¿›åçš„ chunks: {files['improved_chunks']}")
        print(f"ğŸ“Š è¯¦ç»†æŠ¥å‘Š: {files['review_report']}")
        
        print("\nâœ¨ å»ºè®®:")
        if summary['improvement_rate'] > 0.3:
            print("ğŸ”§ æœ‰è¾ƒå¤šé¡µé¢éœ€è¦æ”¹è¿›ï¼Œå»ºè®®æ£€æŸ¥åŸå§‹ OCR è´¨é‡")
        elif summary['improvement_rate'] > 0.1:
            print("ğŸ‘ éƒ¨åˆ†é¡µé¢æœ‰æ”¹è¿›ï¼Œæ•´ä½“è´¨é‡ä¸é”™")
        else:
            print("ğŸ¯ è´¨é‡å¾ˆå¥½ï¼Œå‡ ä¹ä¸éœ€è¦æ”¹è¿›")
        
        if summary['average_confidence'] < 70:
            print("âš ï¸  å¹³å‡ç½®ä¿¡åº¦è¾ƒä½ï¼Œå»ºè®®æ£€æŸ¥æ–‡æ¡£è´¨é‡æˆ–è°ƒæ•´å‚æ•°")
        
        return 0
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­å¤„ç†")
        return 1
    except Exception as e:
        print(f"\nâŒ å¤„ç†å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    sys.exit(main()) 